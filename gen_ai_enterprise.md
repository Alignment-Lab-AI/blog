---
title: "Using Generative AI in the Enterprise"
thumbnail: /blog/assets/153_gen-ai-enterprise/thumbnail.png
authors:
- user: rajistics
- user: sasha
---

## Using Generative AI in the Enterprise

Anyone who has played with models like ChatGPT or Bard, or is following the discussion around them, might be inclined to jump to speculation about how it might be transformative. But the ease of use of a tool like ChatGPT can obscure what is going on under the hood. In order to know how to really make use of them, it's necessary to understand them. This article aims to provide some context about what these models are, how they differ from prior models, how they came to be, how they work, and their real promise and pitfalls.

Artificial Intelligence (AI) can be broadly defined as the ability of machines to perform tasks commonly associated with human intelligence. AI technologies have existed for almost 70 years now, starting in the 1950s and 60s with rule-based AI systems, which essentially leveraged thousands of rules ("if \_\_, then \_\_") in order to make predictions and classify inputs. This includes the famous ELIZA system, released in 1966, a chatbot therapist that was able to maintain the semblance of a very basic, albeit convincing, conversation, providing advice and asking questions based on user input. The people interacting with ELIZA were so convinced of its realism that they came to unconsciously assume it has human behavior qualities like empathy and intelligence, a phenomenon that was coined the "ELIZA effect". Other rule-based systems that had huge success in the late 20th century included expert systems like IBM Watson, which used a combination of hand-written rules and structured knowledge bases in order to win Jeopardy in 2011.

<p align="center">
 <br>
 <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/gen-ai-enterprise/eliza.png" alt="transcript of ELIZA conversation (image source: Wikipedia)" />
</p>



Fast forward 12 years, and systems like ChatGPT are undoubtedly more sophisticated and their outputs are more realistic, but the underlying tasks and purposes are largely the same as those of Eliza and Watson: they can hold realistic conversations that can imitate intelligence and they can be good at finding answers to common trivia questions. But while previous generations of AI models were trained largely based on supervised approaches - meaning that answers were manually provided to them via rules or knowledge bases - newer generations of AI models are more dependent on large, unstructured, sources of data. These generative models are mostly trained in an "unsupervised" way, on millions of documents: web pages, Wikipedia articles and books, with the task of predicting the next word in a sentence. This enables them to create a representation of certain aspects of human knowledge by learning statistical patterns that are present in the data. In practice, this means that they are truly spectacular at generating plausible-sounding language (or, in the words of Neil Gaiman,['information-shaped sentences'](https://twitter.com/neilhimself/status/1639610373115375616?lang=en)). Models like this have been used in applications such as Web search and automatic translation by big tech companies for the last five years, and they have made these tools more accurate: one example of this progress is how good Google Translate has gotten in the last few years.

The most recent generations of these models, such as ChatGPT and Bard, are called "generative" due to their ability to generate new content in an open-ended way: whereas previous models were restricted to the categories provided to them during training (think of a voice typing assistant, which is trained to transcribe what is being said out loud), generative models can create new content based on a query or prompt. These models are the ones that have been making waves in the last 6 months and causing a flurry of hype and discussion. These types of interaction-based models also have a final training step: after initially being trained on a large amount of text data, they are "instructed" by human workers, who interact with these systems for thousands or hours in order to make their outputs more realistic. This is a new approach to training AI models, called Reinforcement Learning from Human Feedback (RLHF) which means that the outputs of AI models are ranked, evaluated and improved by human beings. This is what gives the systems such realistic output and makes it seem like they're actually capable of maintaining a realistic conversation.

These recent models are also upending traditional AI models. [Armen Aghajanyan of Meta points](https://youtu.be/7Q7S0zmLxR8?t=2198) out how academia used to study Natural Language Processing (NLP) in various niche domains like semantic parsing, dialog, reasoning, question answer, reading comprehension, and summarization. Each of these domains used different techniques and models. However, the latest generative AI models, such as OpenAI's GPT-4, are providing state-of-the-art performance across multiple domains using one model. Armin suggests that this makes traditional Natural Language Processing methods obsolete. Practitioners are very enthusiastic about generative AI models, because now they can learn and use one model across a diverse set of tasks.

## Generative AI in the Enterprise

Generative AI hype is arguably at its most feverish at enterprises. Commentators issue warnings and predictions about how AI will change everything; executives in various industries are under pressure to use it to improve productivity; and startups are "pivoting to AI" in an effort to catch the wave of hype and ride it to better investment deals.

## First Uses for Generative AI Models

The first applications for Generative AI models are typically based on established NLP workflows. A call center analytical team may have used several separate NLP models to summarize and categorize customer conversations. It's now possible to replace those several separate models with a single generative model. While initially appealing, Matthew Honnibal suggests these [LLM Maximalist](https://explosion.ai/blog/against-llm-maximalism) approaches can be counterproductive. The advantage of separate NLP models is it makes it easier to optimize or troubleshoot workflows. Consequently, most teams have been very selective in replacing existing NLP workflows with a generative model.

To gauge the effectiveness of an AI model, it's best to start with how it is trained. Generative AI models like OpenAI's GPT-3 are trained on general public data sources like Wikipedia. They have widespread appeal for performing diverse tasks, such as translation, writing poetry, or building an exercise plan. When it comes to specific domains of knowledge, these models often have shallow knowledge. For example, words like strike, bond, or market have different meanings to a consumer versus a financial analyst. [Leippold found that](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4337182) a smaller specialized model trained on financial data provided a 15% accuracy improvement over the general purpose OpenAI GPT-3 model in gauging financial sentiment. Many enterprise use cases require specialized knowledge and this has limited the use of the general purpose Generative AI models within enterprises.

Enterprise use cases are often focused on specific subject matter such as healthcare or finance. Enterprises also have their own company specific jargon around their products and services along with expectations on the proper way to communicate to their customers. Customer support teams don't want models that imitate a reddit conversational style. All of this is leading to enterprises looking for ways to incorporate their knowledge and practices into Generative AI models. The current methods for adding knowledge are expensive and resource intensive. As it becomes easier to incorporate knowledge, generative AI will grow in usefulness for enterprises.

## The Next Wave of Generative AI

While the current wave of Generative AI demonstrates the potential of this technology, we expect the second wave of generative AI to be transformational. Transformation happens when we don't plug generative AI models into existing use cases, but build use cases around Generative AI models. This will occur as teams mature in their understanding of generative AI along with better tooling for managing generative AI. While we can't identify those revolutionary use cases today, there are several likely components, which we describe below.

First, using generative AI models as a natural language gateway to connect to data sources and APIs, since these models are enormously powerful in understanding the nuance of language and expressing complexity. The generative ability lets generative AI model text into other forms. An example of this is using a language model to convert a text description, like "Find all users who have spent over $1000", to SQL code like, "SELECT \* FROM users WHERE purchase \> 1000". Developers are using this ability to connect to a variety of other tools from calculators to an Instacart's shopping list.

Second, is the ability for generative AI models to identify relevant tasks to complete a goal. Early experiments have shown that generative AI models given a request can generate a plan with a list of tasks, and then start solving each task independently. A request could be a report analyzing the creditworthiness of a company. To build this report, a model would plan and generate tasks based on many factors such as the company's size, geographic location, and domain it operates. Once it identified its plan it could then analyze, summarize, and pull all the necessary information together and make a creditworthiness assessment. An analyst could then review the final report and access all the references to verify and augment the automated report. There are early applications, such as AutoGPT, that have shown promise in this goal oriented approach. More recently, the Transformers library released [Transformers Agents](https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/agent), an open source API that can use any open source or closed source LLM to take a goal, break it into tasks, and then autonomously solve each task.

Finally, generative AI models are not only about automation. They also have the abiity to augment and improve existing processes and structures. [Sal Khan showed how](https://www.ted.com/talks/sal_khan_how_ai_could_save_not_destroy_education/c) Khan Academy is using OpenAI's GPT-4 generative model in a product called Khanmigo to help improve the learning process. Khanmigo was trained to push students to think more deeply, rather than just giving an answer. And if a student was stuck, Khanmigo provided gentle guidance to help the student. Khanmigo highlights just how we can use generative AI to improve the educational experience.

## Abundant AI Landscape

The sudden appearance of OpenAI everywhere has led many to believe that they would dominate Generative AI. The thinking was that OpenAI's talent and ability to build cutting-edge models was a step beyond any other competitors. However, there is reason to doubt that line of thinking. First, AI talent has been distributed to a number of other companies (although a relatively small number of companies). We see models emerging from Google and Meta, along with startups like Anthropic rivaling OpenAI's models. Second, the amount and quality of open-source models is catching up. Hugging Face has developed Generative AI models with the community, in the open, like [BigScience's multilingual BLOOM](https://huggingface.co/bigscience/bloom) LLM, or [BigCode's code generation model StarCoder](https://huggingface.co/bigcode/starcoder). We went from ChatGPT at the beginning of 2023, to now over 50 open-source alternative LLMs by mid-2023 (you can compare their performances on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)).

This diversity of generative AI models is valuable for enterprises and should be embraced.

The reasons for this are multiple: first off, a general language model won't understand the specifics of your business. It has not been trained on your proprietary data. Similar to solving an individual task, enterprises will prefer an AI model based on their enterprise data to solve their enterprises' tasks. Recently, Bloomberg [publicly shared their journey](https://arxiv.org/abs/2303.17564) in building their own generative model based on their proprietary data. They use terabytes of their own data along with a 64 GPU cluster for several months to fully build their own model. Bloomberg felt the gains from having their own generative language model were worth being an early adopter in building their own model. Luckily, improvements in training using fine tuning are coming, so models can be built based around proprietary data in hours and days instead of months.

Second, a diverse set of generative AI models provide a wider set of tradeoffs in their ability to solve tasks accurately, cheaply, and quickly. If your customers are using a chatbot to check balances on their account, is it necessary that they are using a chatbot that is capable of passing a medical licensing test? For some tasks, a smaller cheaper language model that provides acceptable performance could be a better solution than a state-of-the-art generative model. We are starting to see this emerge with language models that are experts in a particular domain, such as Google's MedPalm2 for healthcare, to lightning quick models that can run on small devices. This marketplace of AI models will allow enterprises to select the model most appropriate for their problems. If you're curious about what hardware open source LLMs require to run, and how fast they can generate text, you can review the [Open LLM Performance Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard).

## What are the risks with generative AI?

The amazing capabilities of generative AI models also come with risks. We identify four major risks of generative AI models to consider to make responsible decisions regarding the use of these models in the real world.

To start with, **interpretability** has been a major challenge in recent generations of AI models – that is, ones that rely on _machine learning_ to learn mathematical representations of the data that they are trained on. Most of these models are, inherently, uninterpretable; in fact, they are often referred to as "black boxes", given their opacity and inherent lack of transparency. Generative models are no exception: in essence, they take large quantities of data and extract patterns and features from it and use these to generate new content. While there have been many proposals to render AI predictions more interpretable, none of these are able to fully shed light on why models made the predictions they did – at best, they are able to find clues or indicators regarding a model's internal logic or reasoning. What this means in practice is that if a model's prediction is inaccurate (e.g. if it generates a falsehood or misrepresents a fact), it is hard to figure out where the issue came from and how to fix it.

Another major risk of building products and services on generative models is that the content that they produce often contains **false or inaccurate information**. This is due to the fact that, by definition, they _generate_ the most likely sequence based on statistical probabilities calculated from their training data. The models themselves have no mechanism for fact-checking the content they produce, no ground truth to compare to. This often results in the generation of very plausible-sounding, but fundamentally false information. If you have ever asked ChatGPT to generate your own biography, you can see this firsthand – most of what it generates is close to the truth (such as the names of universities similar to the ones you went to, or positions that resemble your own), but is actually mostly misinformed. There are different safeguards and fact-checking operations that can be added on top of language models to verify the content that they produce – which has been added in Bing Search for instance – but this takes a lot of engineering efforts to deploy, and replies upon an existing knowledge base or set of facts, which can change over time, making it hard to maintain in practice.

Another systemic issue that has plagued generative AI models are the **biases** that they encode: what is referred to by the term "bias" are cases when models generate predictions that are systemically prejudiced against given populations, groups or characteristics. There has been much progress made in understanding the way AI models encode biases (for instance, the groundbreaking [Gender Shades](http://gendershades.org/)project), but there continues to be much debate around how to address them. For example, if, when prompted to generate an image of a manager, an text-to-image model like MidJourney or Dall-E 2 generates only images of individuals that resemble White men, whereas a prompt to generate an image of a nurses or a social worker mostly results in non-White and non-male appearing individuals (in fact, this was what we found in our [recent work](https://huggingface.co/spaces/society-ethics/StableBias) that explores biases in generative AI models), to what extent is this problematic? Some do argue that this is, in fact, a representation of the biases that exist in society and are acceptable as such, while others insist that operationalizing these biases in AI models contributes to further perpetuating and amplifying them. The bottom line is that the biases of generative AI models are sparsely documented and under-explored, And they can come to the surface when these models are deployed in products and services.

<p align="center">
 <br>
 <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/gen-ai-enterprise/average-faces.png" alt="average faces for the profession 'manager', generated by the Average Diffusion Faces tool" />
</p>



A final notable risk factor - and topic of much debate - is the legal status of generative AI models. While not many legal precedents have been established thus far, there are several [ongoing court cases](https://techcrunch.com/2023/01/27/the-current-legal-cases-against-generative-ai-are-just-the-beginning/) that question whether training AI models on data that is primarily harvested from the Internet constitutes fair use, and to what extent it infringes upon the copyright of the creators of the images and the text used for training AI models. The legal status of training data is currently murky at best, and the extent to which AI models truly memorize the data they were trained on (versus simply generating content that resembles it) is still to be determined. There are also several ongoing initiatives to establish [opt-in and opt out mechanisms with regards to AI training data](https://slate.com/technology/2023/03/how-holly-herndon-and-mathew-dryhurst-brokered-an-a-i-deal-with-stable-diffusion.html), as well as to develop [watermarking approaches](https://www.technologyreview.com/2023/01/27/1067338/a-watermark-for-chatbots-can-spot-text-written-by-an-ai/) in order to indicate that content is generated using AI models. What this means in practice is that there are many unresolved legal issues that can ultimately impact products and services that are built based on generative models given that these models' legal status is still being debated in many jurisdictions the world over.

## Recommendations

It's an exciting time within this space. There are transformational impacts with generative AI models along with significant risks to consider. Our recommendations are the following:

Generative AI will be revolutionary, but that does not mean one needs to jump on the bandwagon at all costs. We have some breathing room in the near term. In fact, it can be very difficult for enterprises to move quickly across many use cases, and exploring the pros and cons of each one is important. The better strategy is not to be a first mover and wait for the second wave, where it's possible to take advantage of these technologies with a far lower risk and cost. Already in the last year, we have moved from a landscape of a single product to several different competitors. We expect many more competing products to emerge over the next few years. Given the high level of interest and investment into Generative AI, it's going to be a buyer's market for products and services.

Don't get intimidated by the frothy hype and carefully evaluate generative AI as any other tool or technology. What is the problem you are solving? What are the risks and benefits of the different tools? What are the alternative ways to solve the problem? Once people fall back on their traditional analytical reasoning, they find Generative AI projects much more manageable. They recognize that Generative AI is only a small part of an overall enterprise AI strategy. Many analytic workloads like analyzing customer churn and forecasting demand do not require Generative AI. Other projects could be handled by a smaller more domain specific model instead of a general-purpose one..

Finally, the risks of Generative AI models are different from analytic models of the past. It's important to have people trained in the risks of these models. Without some education into AI and how it works, people cannot effectively evaluate the risks. Taking the time to understand how these models are trained, how they work and when they fail is an important part of integrating them into existing workflows and developing new ones. And being thoughtful and cautious about adopting new types of generative models that haven't been extensively tested in different industries and contexts is important in order to strive towards responsible and ethical innovation.
